{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3ab800b",
   "metadata": {},
   "source": [
    "# n2c2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e1f080",
   "metadata": {},
   "source": [
    "## Checking the annotations file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7096d94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n2c2_ann = [line.strip().split(\"\\t\") for line in open(\"Downloads/training_20180910/100035.ann\", \"r\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7c388b9c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Frequency', '15066', '15076;15077', '15095']\n",
      "['Frequency', '15385', '15390;15391', '15398']\n",
      "['Form', '15460', '15473;15474', '15483']\n",
      "['Dosage', '15598', '15601;15602', '15605']\n",
      "['Frequency', '15955', '15957;15958', '15983']\n",
      "['Frequency', '16314', '16336;16337', '16343']\n",
      "['Frequency', '16620', '16632;16633', '16649']\n",
      "['Frequency', '16839', '16850;16851', '16860']\n",
      "['Reason', '9314', '9323;9324', '9331']\n",
      "['Frequency', '15164', '15176;15177', '15184']\n",
      "['Frequency', '15314', '15328;15329', '15333']\n",
      "['Frequency', '14964', '14978;14979', '14993']\n",
      "['Frequency', '16404', '16416;16417', '16423']\n"
     ]
    }
   ],
   "source": [
    "for i in n2c2_ann:\n",
    "    sent=i[1].split()\n",
    "    try:\n",
    "        s=int(sent[-1])\n",
    "        if len(sent) > 3:\n",
    "            print(sent)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "be78dc99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['T1', 'Reason', '10179:10197', 'recurrent seizures'], ['T3', 'Drug', '10227:10233', 'ativan'], ['T5', 'Route', '10240:10242', 'IM'], ['T6', 'Drug', '10455:10465', 'Topiramate'], ['T7', 'Strength', '10466:10470', '25mg'], ['T8', 'Route', '10471:10473', 'PO'], ['T9', 'Frequency', '10474:10477', 'BID'], ['T10', 'Duration', '10495:10497', 'PM'], ['T11', 'Strength', '10515:10519', '50mg'], ['T12', 'Route', '10520:10522', 'po']]\n"
     ]
    }
   ],
   "source": [
    "n2c2_result = []\n",
    "for item in n2c2_ann:\n",
    "    words = item[1].split()\n",
    "    try:\n",
    "    # check if the second last word is a number\n",
    "        if words[-2].isdigit() and words[-1].isdigit():\n",
    "            start, end = map(int, [words[-2], words[-1]])\n",
    "            # check if the length of the string in the 3rd position is equal to the difference between the two numbers\n",
    "            if end - start == len(item[2]):\n",
    "                n2c2_result.append([item[0], words[0], f\"{start}:{end}\", item[2]])\n",
    "        \n",
    "    except:\n",
    "        continue\n",
    "\n",
    "print(n2c2_result[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab6846a",
   "metadata": {},
   "source": [
    "## Merging word annotations and text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8077415a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def load_text(file_path):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        return file.read()\n",
    "\n",
    "def load_annotations(file_path):\n",
    "    annotations = []\n",
    "    with open(file_path, \"r\") as file:\n",
    "        for line in file:\n",
    "            items = line.strip().split(\"\\t\")\n",
    "            try:\n",
    "                annotations.append([items[0], items[1], items[2].split()])\n",
    "            except:\n",
    "                continue\n",
    "    return annotations\n",
    "\n",
    "def annotated_text(text, annotations):\n",
    "    positions = []\n",
    "    for annotation in annotations:\n",
    "        if annotation[0].startswith(\"T\"):\n",
    "            try:\n",
    "                start, end = map(int, annotation[1].split()[1:])\n",
    "                positions.append((start, end, annotation[1].split(\" \")[0], annotation[2]))\n",
    "            except:\n",
    "                start, end = int(annotation[1].split()[1]), int(annotation[1].split()[-1])\n",
    "                positions.append((start, end, annotation[1].split(\" \")[0], annotation[2]))\n",
    "\n",
    "    # sort the positions list by start index\n",
    "    positions.sort(key=lambda x: x[0])\n",
    "\n",
    "    annotated_text = \"\"\n",
    "    start = 0\n",
    "    for position in positions:\n",
    "        annotated_text += text[start:position[0]]\n",
    "        annotated_text += f\"<{position[2]}> {str(position[3])} </{position[2]}> \"\n",
    "        start = position[1]\n",
    "    annotated_text += text[start:]\n",
    "    return annotated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "063c8565",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bart_to_ner(text):\n",
    "    tokens = text.split()\n",
    "    ner_tags = []\n",
    "    entity_start = False\n",
    "    current_entity = \"\"\n",
    "    current_tag = 0\n",
    "    \n",
    "    for token in tokens:\n",
    "        if \"<\" in token and \">\" in token:\n",
    "            if \"/\" in token:\n",
    "                entity_start = False\n",
    "                ner_tags.append((current_entity, current_tag))\n",
    "                current_entity = \"\"\n",
    "                current_tag = 0\n",
    "            else:\n",
    "                entity_start = True\n",
    "                current_tag = token.split(\"<\")[1].split(\">\")[0]\n",
    "        elif entity_start:\n",
    "            current_entity += token + \" \"\n",
    "        else:\n",
    "            ner_tags.append((token, 0))\n",
    "    ner_taggers=[(x[0].replace(\"['\", \"\").replace(\"']\", \"\").strip(), x[1]) for x in ner_tags]        \n",
    "    return ner_taggers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24e5ee4",
   "metadata": {},
   "source": [
    "## Turning annotated text into NER annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bfb0f592",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "ner_tags_in_txt =[\"Drug\", \"Strength\", \"Form\", \"Frequency\", \"Route\", \"Dosage\", \"Reason\", \"ADE\", \"Duration\"]\n",
    "\n",
    "def nerring_da_txt(ann_txt):\n",
    "    \n",
    "    nerred_txt = []\n",
    "\n",
    "    for l, i in enumerate(ann_txt):\n",
    "        if i[1] in ner_tags_in_txt:\n",
    "            aa=re.findall(r'\\b\\w+\\b', i[0])\n",
    "            for h, j in enumerate(aa):\n",
    "                if h==0:\n",
    "                    nerred_txt.append((j, \"B-\"+i[1]))\n",
    "                else:\n",
    "                    nerred_txt.append((j, \"I-\"+i[1]))\n",
    "        #elif i[1] == \"ADE\":\n",
    "         #   aa=re.findall(r'\\b\\w+\\b', i[0])\n",
    "          #  for h, j in enumerate(aa):\n",
    "           ##     if h==0:\n",
    "             #       nerred_txt.append((j, \"B-\"+i[1]))\n",
    "              #  else:\n",
    "               #     nerred_txt.append((j, \"I-\"+i[1]))\n",
    "        else:\n",
    "            aa=re.findall(r'\\b\\w+\\b', i[0])\n",
    "            for j in aa:\n",
    "                nerred_txt.append((j, 0))\n",
    "                \n",
    "    return nerred_txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9d3ffdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = load_text(\"Downloads/training_20180910/114144.txt\")\n",
    "annotations = load_annotations(\"Downloads/training_20180910/114144.ann\")\n",
    "txt = annotated_text(text, annotations)\n",
    "almost = bart_to_ner(txt)\n",
    "end_try= nerring_da_txt(almost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9ee120",
   "metadata": {},
   "source": [
    "## ALL the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2717997b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[110727, 101372, 106384]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "dir_path = \"Downloads/training_20180910/\"\n",
    "\n",
    "filenames = os.listdir(dir_path)\n",
    "\n",
    "ids = [int(f.split(\".\")[0]) for f in filenames if f.endswith(\".txt\")]\n",
    "\n",
    "ids[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d599e9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_texts_eng=[]\n",
    "for nmbr in ids:\n",
    "    try:\n",
    "        text = load_text(f\"{dir_path}{nmbr}.txt\")\n",
    "        annotations = load_annotations(f\"{dir_path}{nmbr}.ann\")\n",
    "        txt = annotated_text(text, annotations)\n",
    "        ner_txt = bart_to_ner(txt)\n",
    "        nerred_txt = nerring_da_txt(ner_txt)\n",
    "\n",
    "        ner_texts_eng.append(nerred_txt)\n",
    "    except:\n",
    "        try:\n",
    "            text = load_text(f\"{dir_path}{nmbr}.txt\")\n",
    "            print(nmbr)\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "45eee4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('output.tsv', 'w', newline='', encoding='utf-8') as tsvfile:\n",
    "    writer = csv.writer(tsvfile, delimiter='\\t')\n",
    "    for i, sublist in enumerate(ner_texts_eng):\n",
    "        for j, (word, label) in enumerate(sublist):\n",
    "            if label == 0:\n",
    "                writer.writerow([word, 0])\n",
    "            else:\n",
    "                writer.writerow([word, label])\n",
    "            if j == len(sublist) - 1 and i != len(nerred_txt) - 1:\n",
    "                writer.writerow([])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "cfbb9e10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3557"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ner_texts_eng[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e472e397",
   "metadata": {},
   "source": [
    "# Estonian stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0c6d9830",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "58510a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tekst= pd.read_csv(\"Downloads/training_data_AD_and_AP_1st_and_2nd.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2c4d9184",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def NER_annotate(text):\n",
    "    adr_pattern = r'<adr> (\\w+) <adr>'\n",
    "    adr_pattern_2 = r'< adr > (\\w+) < adr >'\n",
    "    \n",
    "    adr_pattern = r'<( adr )>'\n",
    "    adr_pattern_2 = r'<(adr)>'\n",
    "    \n",
    "    drg_pattern = r'<( drg )>'\n",
    "    drg_pattern_2 = r'<(drg)>'\n",
    "    \n",
    "    annot_pattern = r'<(\\w+)>'\n",
    "    annot_pattern_2 = r'< (\\w+) >'\n",
    "\n",
    "    adr_matches = re.findall(adr_pattern, text)\n",
    "    adr_matches_2 = re.findall(adr_pattern_2, text)\n",
    "    \n",
    "    adr_annot_matches =[i.strip() for i in re.findall(adr_pattern, text)]\n",
    "    adr_annot_matches_2 = re.findall(adr_pattern_2, text)\n",
    "    \n",
    "    drg_annot_matches =[i.strip() for i in re.findall(drg_pattern, text)]\n",
    "    drg_annot_matches_2 = re.findall(drg_pattern_2, text)\n",
    "    \n",
    "    annot_matches = re.findall(annot_pattern, text)\n",
    "    annot_matches_2 = re.findall(annot_pattern_2, text)\n",
    "    \n",
    "    words = re.findall(r'\\b\\w+\\b', text)\n",
    "\n",
    "    adr_in_annotation = False  # Flag to keep track of whether currently inside an annotation\n",
    "    drg_in_annotation = False\n",
    "    the_first = True\n",
    "    ner_annotated_text = []\n",
    "\n",
    "    for word in words:\n",
    "        if adr_in_annotation:\n",
    "            if word in adr_annot_matches or word in adr_annot_matches_2:\n",
    "                adr_in_annotation = not adr_in_annotation\n",
    "                the_first= not the_first\n",
    "                continue\n",
    "            elif word in annot_matches or word in annot_matches_2:\n",
    "                continue \n",
    "            elif the_first:\n",
    "                ner_annotated_text.append((word, 'B-ADE'))\n",
    "                the_first = not the_first\n",
    "            else:\n",
    "                ner_annotated_text.append((word, 'I-ADE'))\n",
    "        elif drg_in_annotation:\n",
    "            if word in drg_annot_matches or word in drg_annot_matches_2:\n",
    "                drg_in_annotation = not drg_in_annotation\n",
    "                the_first= not the_first\n",
    "                continue\n",
    "            elif word in annot_matches or word in annot_matches_2:\n",
    "                continue \n",
    "            elif the_first:\n",
    "                ner_annotated_text.append((word, 'B-Drug'))\n",
    "                the_first = not the_first\n",
    "            else:\n",
    "                ner_annotated_text.append((word, 'I-Drug'))\n",
    "        else:\n",
    "            if word in adr_annot_matches or word in adr_annot_matches_2:\n",
    "                adr_in_annotation = not adr_in_annotation\n",
    "                continue\n",
    "            if word in drg_annot_matches or word in drg_annot_matches_2:\n",
    "                drg_in_annotation = not drg_in_annotation\n",
    "                continue\n",
    "            elif word in annot_matches or word in annot_matches_2:\n",
    "                continue      \n",
    "            else:\n",
    "                ner_annotated_text.append((word, 0))\n",
    "\n",
    "    return ner_annotated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bb99e76a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('CLS', 0),\n",
       " ('es', 0),\n",
       " ('muutus', 0),\n",
       " ('jarjest', 0),\n",
       " ('tundlikumaks', 0),\n",
       " ('valistele', 0),\n",
       " ('sundmustele', 0),\n",
       " ('toostressile', 0),\n",
       " ('seetottu', 0),\n",
       " ('alates', 0),\n",
       " ('oktoobrist', 0),\n",
       " ('uues', 0),\n",
       " ('ti', 'B-Drug'),\n",
       " ('kvetiapiin', 'I-Drug'),\n",
       " ('tostetud', 0),\n",
       " ('pt', 0),\n",
       " ('olnud', 0),\n",
       " ('tvl', 0),\n",
       " ('il', 0),\n",
       " ('et', 0)]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NER_annotate(tekst.iloc[3][\"verified\"])[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "id": "7a73ac92",
   "metadata": {},
   "outputs": [],
   "source": [
    "tekstid= [NER_annotate(tekst.iloc[i][\"verified\"]) for i in range(len(tekst))][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "id": "8678b562",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 ('fluanxol', 'I-Drug')\n",
      "14 ('risperidon', 'I-Drug')\n",
      "15 ('lorasepaam', 'I-Drug')\n",
      "24 ('risperidoon', 'I-Drug')\n",
      "44 ('bromazepam', 'I-Drug')\n",
      "62 ('haloperidool', 'I-Drug')\n",
      "99 ('lorasepaam', 'I-Drug')\n",
      "103 ('aripiprasool', 'I-Drug')\n",
      "106 ('abilify', 'I-Drug')\n",
      "113 ('haloperidol', 'I-Drug')\n",
      "120 ('alprazolam', 'I-Drug')\n",
      "142 ('diasepaam', 'I-Drug')\n",
      "161 ('diasepaam', 'I-Drug')\n",
      "168 ('risperidoon', 'I-Drug')\n",
      "412 ('sertraliin', 'I-Drug')\n",
      "413 ('sertraliin', 'I-Drug')\n",
      "424 ('Escitalopram', 'I-Drug')\n",
      "429 ('estsitalopraam', 'I-Drug')\n",
      "750 ('Sertralin', 'I-Drug')\n",
      "751 ('Sertralin', 'I-Drug')\n",
      "767 ('sertraliin', 'I-Drug')\n",
      "812 ('i', 'I-Drug')\n",
      "842 ('Sertralini', 'I-Drug')\n",
      "843 ('Sertralini', 'I-Drug')\n",
      "844 ('Sertralini', 'I-Drug')\n",
      "845 ('Sertralini', 'I-Drug')\n",
      "921 ('venlafaxini', 'I-Drug')\n",
      "922 ('venlafaxini', 'I-Drug')\n",
      "925 ('estsitaloprami', 'I-Drug')\n",
      "950 ('escitalopram', 'I-Drug')\n",
      "951 ('escitalopram', 'I-Drug')\n",
      "984 ('Valdoxan', 'I-Drug')\n",
      "985 ('Valdoxan', 'I-Drug')\n",
      "986 ('Valdoxan', 'I-Drug')\n",
      "991 ('Escitalopram', 'I-Drug')\n",
      "992 ('Escitalopram', 'I-Drug')\n",
      "997 ('Cipralex', 'I-Drug')\n",
      "998 ('Cipralex', 'I-Drug')\n",
      "999 ('Cipralex', 'I-Drug')\n",
      "1043 ('citaloprami', 'I-Drug')\n",
      "1045 ('escitalopram', 'I-Drug')\n",
      "1048 ('Valdoxan', 'I-Drug')\n",
      "1049 ('Valdoxan', 'I-Drug')\n",
      "1050 ('Valdoxan', 'I-Drug')\n",
      "1099 ('estsitalopraam', 'I-Drug')\n"
     ]
    }
   ],
   "source": [
    "for s, i in enumerate(tekstid[700:1800]):\n",
    "    for l, k in enumerate(i):\n",
    "        if k[1] ==\"I-Drug\" :\n",
    "            print(s, i[l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "id": "ba187350",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('est_output.tsv', 'w') as f:\n",
    "    for sublist in tekstid:\n",
    "        for tuple in sublist:\n",
    "            f.write('\\t'.join(map(str, tuple)) + '\\n')\n",
    "        f.write('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
