{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47cf01ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/space/home/shuva/miniconda3/envs/roberta/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-03-08 14:56:19.073031: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-08 14:56:34.380936: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-08 14:56:34.381077: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-08 14:56:34.381091: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import XLMRobertaForTokenClassification, XLMRobertaTokenizerFast, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from transformers import AutoModelForTokenClassification\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datasets import Dataset, load_dataset\n",
    "from datasets import load_metric\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25d79b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seqeval in /gpfs/space/home/shuva/miniconda3/envs/roberta/lib/python3.8/site-packages (1.2.2)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /gpfs/space/home/shuva/miniconda3/envs/roberta/lib/python3.8/site-packages (from seqeval) (1.2.1)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /gpfs/space/home/shuva/miniconda3/envs/roberta/lib/python3.8/site-packages (from seqeval) (1.23.5)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /gpfs/space/home/shuva/miniconda3/envs/roberta/lib/python3.8/site-packages (from scikit-learn>=0.21.3->seqeval) (3.1.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /gpfs/space/home/shuva/miniconda3/envs/roberta/lib/python3.8/site-packages (from scikit-learn>=0.21.3->seqeval) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /gpfs/space/home/shuva/miniconda3/envs/roberta/lib/python3.8/site-packages (from scikit-learn>=0.21.3->seqeval) (1.2.0)\n",
      "Requirement already satisfied: datasets in /gpfs/space/home/shuva/miniconda3/envs/roberta/lib/python3.8/site-packages (2.10.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /gpfs/space/home/shuva/miniconda3/envs/roberta/lib/python3.8/site-packages (from datasets) (2.28.2)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /gpfs/space/home/shuva/miniconda3/envs/roberta/lib/python3.8/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /gpfs/space/home/shuva/miniconda3/envs/roberta/lib/python3.8/site-packages (from datasets) (11.0.0)\n",
      "Requirement already satisfied: xxhash in /gpfs/space/home/shuva/miniconda3/envs/roberta/lib/python3.8/site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /gpfs/space/home/shuva/miniconda3/envs/roberta/lib/python3.8/site-packages (from datasets) (0.12.1)\n",
      "Requirement already satisfied: responses<0.19 in /gpfs/space/home/shuva/miniconda3/envs/roberta/lib/python3.8/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /gpfs/space/home/shuva/miniconda3/envs/roberta/lib/python3.8/site-packages (from datasets) (2023.1.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /gpfs/space/home/shuva/miniconda3/envs/roberta/lib/python3.8/site-packages (from datasets) (4.64.1)\n",
      "Requirement already satisfied: multiprocess in /gpfs/space/home/shuva/miniconda3/envs/roberta/lib/python3.8/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: pandas in /gpfs/space/home/shuva/miniconda3/envs/roberta/lib/python3.8/site-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /gpfs/space/home/shuva/miniconda3/envs/roberta/lib/python3.8/site-packages (from datasets) (1.23.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /gpfs/space/home/shuva/miniconda3/envs/roberta/lib/python3.8/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: aiohttp in /gpfs/space/home/shuva/miniconda3/envs/roberta/lib/python3.8/site-packages (from datasets) (3.8.4)\n",
      "Requirement already satisfied: packaging in /gpfs/space/home/shuva/miniconda3/envs/roberta/lib/python3.8/site-packages (from datasets) (23.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /gpfs/space/home/shuva/miniconda3/envs/roberta/lib/python3.8/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /gpfs/space/home/shuva/miniconda3/envs/roberta/lib/python3.8/site-packages (from aiohttp->datasets) (22.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /gpfs/space/home/shuva/miniconda3/envs/roberta/lib/python3.8/site-packages (from aiohttp->datasets) (1.8.2)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /gpfs/space/home/shuva/miniconda3/envs/roberta/lib/python3.8/site-packages (from aiohttp->datasets) (3.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /gpfs/space/home/shuva/miniconda3/envs/roberta/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /gpfs/space/home/shuva/miniconda3/envs/roberta/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /gpfs/space/home/shuva/miniconda3/envs/roberta/lib/python3.8/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: filelock in /gpfs/space/home/shuva/miniconda3/envs/roberta/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /gpfs/space/home/shuva/miniconda3/envs/roberta/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /gpfs/space/home/shuva/miniconda3/envs/roberta/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /gpfs/space/home/shuva/miniconda3/envs/roberta/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /gpfs/space/home/shuva/miniconda3/envs/roberta/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (1.26.14)\n",
      "Requirement already satisfied: pytz>=2020.1 in /gpfs/space/home/shuva/miniconda3/envs/roberta/lib/python3.8/site-packages (from pandas->datasets) (2022.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /gpfs/space/home/shuva/miniconda3/envs/roberta/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /gpfs/space/home/shuva/miniconda3/envs/roberta/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install seqeval\n",
    "! pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "433462d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.environ[\"WANDB_API_KEY\"] = api_key\n",
    "os.environ[\"WANDB_ENTITY\"] = \"Hendrik\"\n",
    "os.environ[\"WANDB_PROJECT\"] = \"health_xlmr\"\n",
    "os.environ[\"WAND_MODE\"] = \"offline\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7ba6ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md\tfine_tuning  output_all_tags.tsv\r\n",
      "est_output.tsv\toutput.tsv   training_data_AD_and_AP_1st_and_2nd.csv\r\n"
     ]
    }
   ],
   "source": [
    "! ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b45cc8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "#unmasker = pipeline('ner', model='xlm-roberta-base')\n",
    "#unmasker(\"Hello I'm a <mask> model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "485b2204",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('data/output_all_tags.tsv', 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=\"\\t\")\n",
    "    output = list(reader)\n",
    "\n",
    "output = [row for row in output]\n",
    "# labeled_tokens = [(token, label) for token, label in output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fbe0c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = []\n",
    "temporary = []\n",
    "\n",
    "for entry in output:\n",
    "    if len(entry) == 2:\n",
    "        text = entry[0]\n",
    "        label = entry[1]\n",
    "        if label == '0':\n",
    "            label = 'O'\n",
    "        if text not in ['SEP', 'CLS']:\n",
    "            temporary.append([text, label])\n",
    "    if len(entry) == 0:\n",
    "        total.append(temporary)\n",
    "        temporary = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be96c15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_ = []\n",
    "\n",
    "for x in total:\n",
    "    for y in x:\n",
    "        label_ = y[1]\n",
    "        labels_.append(label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0551a0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nplabels = np.array(labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d291bb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = np.unique(nplabels, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cfd23b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B-ADE - 946\n",
      "B-Dosage - 4220\n",
      "B-Drug - 16025\n",
      "B-Duration - 592\n",
      "B-Form - 6647\n",
      "B-Frequency - 6272\n",
      "B-Reason - 3776\n",
      "B-Route - 5469\n",
      "B-Strength - 6686\n",
      "I-ADE - 774\n",
      "I-Dosage - 3584\n",
      "I-Drug - 4205\n",
      "I-Duration - 1003\n",
      "I-Form - 2907\n",
      "I-Frequency - 9411\n",
      "I-Reason - 3119\n",
      "I-Route - 426\n",
      "I-Strength - 7150\n",
      "O - 544824\n"
     ]
    }
   ],
   "source": [
    "counts_ = unique_values[1]\n",
    "names_ = unique_values[0]\n",
    "\n",
    "for k in range(len(counts_)):\n",
    "    print(names_[k], \"-\", counts_[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e644063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-ADE', 'B-Dosage', 'B-Drug', 'B-Duration', 'B-Form', 'B-Frequency', 'B-Reason', 'B-Route', 'B-Strength', 'I-ADE', 'I-Dosage', 'I-Drug', 'I-Duration', 'I-Form', 'I-Frequency', 'I-Reason', 'I-Route', 'I-Strength', 'O']\n"
     ]
    }
   ],
   "source": [
    "label_list = list(names_)\n",
    "print(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d02e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(list(names_))\n",
    "list(le.classes_)\n",
    "le.transform(['B-Drug'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6fa8d14a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the XLM-RoBERTa model and tokenizer\n",
    "label2id = {label: i for i, label in enumerate(label_list)}\n",
    "id2label = {i: label for i, label in enumerate(label_list)}\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained('xlm-roberta-base', num_labels=len(label_list), id2label=id2label, label2id=label2id)\n",
    "tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base', max_length=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7a91c45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303\n",
      "227 76\n"
     ]
    }
   ],
   "source": [
    "print(len(total))\n",
    "training_total = total[:int(len(total)*0.75)]\n",
    "test_total = total[int(len(total)*0.75):]\n",
    "\n",
    "print(len(training_total), len(test_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "57d762ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_tokens_and_ner_tags(texts):\n",
    "    return pd.concat([get_tokens_and_ner_tags(texts)]).reset_index().drop('index', axis=1)\n",
    "    \n",
    "def get_tokens_and_ner_tags(texts):\n",
    "    all_tokens = []\n",
    "    all_entities = []\n",
    "    for row in texts:\n",
    "        tokens = [x[0] for x in row]\n",
    "        entities = [x[1] for x in row]\n",
    "        \n",
    "        all_tokens.append(tokens)\n",
    "        all_entities.append(entities)\n",
    "    return pd.DataFrame({'tokens': all_tokens, 'ner_tags': all_entities})\n",
    "\n",
    "def get_un_token_dataset(train_directory, test_directory):\n",
    "    train_df = get_all_tokens_and_ner_tags(train_directory)\n",
    "    test_df = get_all_tokens_and_ner_tags(test_directory)\n",
    "    train_dataset = Dataset.from_pandas(train_df)\n",
    "    test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "    return (train_dataset, test_dataset)\n",
    "\n",
    "train_dataset, test_dataset = get_un_token_dataset(training_total, test_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "902fc9a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tokens', 'ner_tags'],\n",
       "    num_rows: 227\n",
       "})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ffe2da05",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {label: i for i, label in enumerate(label_list)}\n",
    "id2label = {i: label for i, label in enumerate(label_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "89193a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    }
   ],
   "source": [
    "task = \"ner\"\n",
    "label_encoding_dict = label2id # taken from code but i switched the variable\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    label_all_tokens = True\n",
    "    tokenized_inputs = tokenizer(list(examples[\"tokens\"]), truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"{task}_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif label[word_idx] == '0':\n",
    "                label_ids.append(0)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label2id[label[word_idx]])\n",
    "            else:\n",
    "                label_ids.append(label2id[label[word_idx]] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "        \n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "train_tokenized_datasets = train_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "test_tokenized_datasets = test_dataset.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "135076c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens 3557\n",
      "ner_tags 3557\n",
      "input_ids 512\n",
      "attention_mask 512\n",
      "labels 512\n"
     ]
    }
   ],
   "source": [
    "train_tokenized_datasets\n",
    "\n",
    "for feature in train_tokenized_datasets.features:\n",
    "    print(feature, len(train_tokenized_datasets[0][feature]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "23a7cc0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(dtype='string', id=None)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# label_names\n",
    "train_tokenized_datasets.features['ner_tags'].feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fbf180",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train_tokenized_datasets[0]['ner_tags'])):\n",
    "    print(train_tokenized_datasets[0]['ner_tags'][i], train_tokenized_datasets[0]['labels'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b3aea46b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# label_list = label2id = {\"0\": 0, \"B-ADE\": 1, \"B-Drug\": 2, \"I-Drug\": 3, \"I-ADE\": 4}\n",
    "try:\n",
    "    wandb.init(mode=\"disabled\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "19a26d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: ner_tags, tokens. If ner_tags, tokens are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "/gpfs/space/home/shuva/miniconda3/envs/roberta/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 227\n",
      "  Num Epochs = 12\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 180\n",
      "  Number of trainable parameters = 277467667\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='180' max='180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [180/180 02:00, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.350879</td>\n",
       "      <td>0.489177</td>\n",
       "      <td>0.044349</td>\n",
       "      <td>0.081324</td>\n",
       "      <td>0.917721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.290707</td>\n",
       "      <td>0.548030</td>\n",
       "      <td>0.523940</td>\n",
       "      <td>0.535714</td>\n",
       "      <td>0.940421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.260630</td>\n",
       "      <td>0.666189</td>\n",
       "      <td>0.547488</td>\n",
       "      <td>0.601034</td>\n",
       "      <td>0.949417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.223075</td>\n",
       "      <td>0.803056</td>\n",
       "      <td>0.556907</td>\n",
       "      <td>0.657706</td>\n",
       "      <td>0.952837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.195461</td>\n",
       "      <td>0.667497</td>\n",
       "      <td>0.631083</td>\n",
       "      <td>0.648780</td>\n",
       "      <td>0.954810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.182257</td>\n",
       "      <td>0.817049</td>\n",
       "      <td>0.643250</td>\n",
       "      <td>0.719807</td>\n",
       "      <td>0.959571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.186796</td>\n",
       "      <td>0.653150</td>\n",
       "      <td>0.708006</td>\n",
       "      <td>0.679473</td>\n",
       "      <td>0.956651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.177357</td>\n",
       "      <td>0.647510</td>\n",
       "      <td>0.729592</td>\n",
       "      <td>0.686104</td>\n",
       "      <td>0.958150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.167734</td>\n",
       "      <td>0.693409</td>\n",
       "      <td>0.722527</td>\n",
       "      <td>0.707669</td>\n",
       "      <td>0.960833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.162745</td>\n",
       "      <td>0.732249</td>\n",
       "      <td>0.724490</td>\n",
       "      <td>0.728349</td>\n",
       "      <td>0.962832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.167753</td>\n",
       "      <td>0.694144</td>\n",
       "      <td>0.730377</td>\n",
       "      <td>0.711800</td>\n",
       "      <td>0.959965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.165680</td>\n",
       "      <td>0.708569</td>\n",
       "      <td>0.736656</td>\n",
       "      <td>0.722340</td>\n",
       "      <td>0.962280</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: ner_tags, tokens. If ner_tags, tokens are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 76\n",
      "  Batch size = 16\n",
      "/gpfs/space/home/shuva/miniconda3/envs/roberta/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: ner_tags, tokens. If ner_tags, tokens are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 76\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: ner_tags, tokens. If ner_tags, tokens are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 76\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: ner_tags, tokens. If ner_tags, tokens are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 76\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: ner_tags, tokens. If ner_tags, tokens are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 76\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: ner_tags, tokens. If ner_tags, tokens are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 76\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: ner_tags, tokens. If ner_tags, tokens are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 76\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: ner_tags, tokens. If ner_tags, tokens are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 76\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: ner_tags, tokens. If ner_tags, tokens are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 76\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: ner_tags, tokens. If ner_tags, tokens are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 76\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: ner_tags, tokens. If ner_tags, tokens are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 76\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: ner_tags, tokens. If ner_tags, tokens are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 76\n",
      "  Batch size = 16\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: ner_tags, tokens. If ner_tags, tokens are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 76\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to xlmr_ner.model\n",
      "Configuration saved in xlmr_ner.model/config.json\n",
      "Model weights saved in xlmr_ner.model/pytorch_model.bin\n",
      "tokenizer config file saved in xlmr_ner.model/tokenizer_config.json\n",
      "Special tokens file saved in xlmr_ner.model/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"test-{task}\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=12,\n",
    "    weight_decay=1e-5,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "\n",
    "metric = load_metric(\"seqeval\")\n",
    "results_collection = []\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [[label_list[p] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels)]\n",
    "    true_labels = [[label_list[l] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels)]\n",
    "    \n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    results_collection.append(results)\n",
    "    return {\"precision\": results[\"overall_precision\"], \"recall\": results[\"overall_recall\"], \"f1\": results[\"overall_f1\"], \"accuracy\": results[\"overall_accuracy\"]}\n",
    "    \n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_tokenized_datasets,\n",
    "    eval_dataset=test_tokenized_datasets,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.evaluate()\n",
    "trainer.save_model('xlmr_ner.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "72004185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ADE': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 92},\n",
       " 'Dosage': {'precision': 0.49038461538461536,\n",
       "  'recall': 0.5151515151515151,\n",
       "  'f1': 0.5024630541871921,\n",
       "  'number': 99},\n",
       " 'Drug': {'precision': 0.8481792717086835,\n",
       "  'recall': 0.9055023923444976,\n",
       "  'f1': 0.8759039629736766,\n",
       "  'number': 1672},\n",
       " 'Duration': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 12},\n",
       " 'Form': {'precision': 0.5185185185185185,\n",
       "  'recall': 0.22580645161290322,\n",
       "  'f1': 0.3146067415730337,\n",
       "  'number': 62},\n",
       " 'Frequency': {'precision': 0.25,\n",
       "  'recall': 0.4084507042253521,\n",
       "  'f1': 0.31016042780748665,\n",
       "  'number': 71},\n",
       " 'Reason': {'precision': 0.2043343653250774,\n",
       "  'recall': 0.24087591240875914,\n",
       "  'f1': 0.22110552763819097,\n",
       "  'number': 274},\n",
       " 'Route': {'precision': 0.7592592592592593,\n",
       "  'recall': 0.6833333333333333,\n",
       "  'f1': 0.7192982456140351,\n",
       "  'number': 120},\n",
       " 'Strength': {'precision': 0.6505376344086021,\n",
       "  'recall': 0.8287671232876712,\n",
       "  'f1': 0.7289156626506025,\n",
       "  'number': 146},\n",
       " 'overall_precision': 0.7085692714231786,\n",
       " 'overall_recall': 0.7366562009419152,\n",
       " 'overall_f1': 0.7223398114296709,\n",
       " 'overall_accuracy': 0.9622800326169871}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_collection[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6694bf0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file xlmr_ner.model/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlmr_ner.model\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"B-ADE\",\n",
      "    \"1\": \"B-Dosage\",\n",
      "    \"2\": \"B-Drug\",\n",
      "    \"3\": \"B-Duration\",\n",
      "    \"4\": \"B-Form\",\n",
      "    \"5\": \"B-Frequency\",\n",
      "    \"6\": \"B-Reason\",\n",
      "    \"7\": \"B-Route\",\n",
      "    \"8\": \"B-Strength\",\n",
      "    \"9\": \"I-ADE\",\n",
      "    \"10\": \"I-Dosage\",\n",
      "    \"11\": \"I-Drug\",\n",
      "    \"12\": \"I-Duration\",\n",
      "    \"13\": \"I-Form\",\n",
      "    \"14\": \"I-Frequency\",\n",
      "    \"15\": \"I-Reason\",\n",
      "    \"16\": \"I-Route\",\n",
      "    \"17\": \"I-Strength\",\n",
      "    \"18\": \"O\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"B-ADE\": 0,\n",
      "    \"B-Dosage\": 1,\n",
      "    \"B-Drug\": 2,\n",
      "    \"B-Duration\": 3,\n",
      "    \"B-Form\": 4,\n",
      "    \"B-Frequency\": 5,\n",
      "    \"B-Reason\": 6,\n",
      "    \"B-Route\": 7,\n",
      "    \"B-Strength\": 8,\n",
      "    \"I-ADE\": 9,\n",
      "    \"I-Dosage\": 10,\n",
      "    \"I-Drug\": 11,\n",
      "    \"I-Duration\": 12,\n",
      "    \"I-Form\": 13,\n",
      "    \"I-Frequency\": 14,\n",
      "    \"I-Reason\": 15,\n",
      "    \"I-Route\": 16,\n",
      "    \"I-Strength\": 17,\n",
      "    \"O\": 18\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading configuration file xlmr_ner.model/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlmr_ner.model\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"B-ADE\",\n",
      "    \"1\": \"B-Dosage\",\n",
      "    \"2\": \"B-Drug\",\n",
      "    \"3\": \"B-Duration\",\n",
      "    \"4\": \"B-Form\",\n",
      "    \"5\": \"B-Frequency\",\n",
      "    \"6\": \"B-Reason\",\n",
      "    \"7\": \"B-Route\",\n",
      "    \"8\": \"B-Strength\",\n",
      "    \"9\": \"I-ADE\",\n",
      "    \"10\": \"I-Dosage\",\n",
      "    \"11\": \"I-Drug\",\n",
      "    \"12\": \"I-Duration\",\n",
      "    \"13\": \"I-Form\",\n",
      "    \"14\": \"I-Frequency\",\n",
      "    \"15\": \"I-Reason\",\n",
      "    \"16\": \"I-Route\",\n",
      "    \"17\": \"I-Strength\",\n",
      "    \"18\": \"O\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"B-ADE\": 0,\n",
      "    \"B-Dosage\": 1,\n",
      "    \"B-Drug\": 2,\n",
      "    \"B-Duration\": 3,\n",
      "    \"B-Form\": 4,\n",
      "    \"B-Frequency\": 5,\n",
      "    \"B-Reason\": 6,\n",
      "    \"B-Route\": 7,\n",
      "    \"B-Strength\": 8,\n",
      "    \"I-ADE\": 9,\n",
      "    \"I-Dosage\": 10,\n",
      "    \"I-Drug\": 11,\n",
      "    \"I-Duration\": 12,\n",
      "    \"I-Form\": 13,\n",
      "    \"I-Frequency\": 14,\n",
      "    \"I-Reason\": 15,\n",
      "    \"I-Route\": 16,\n",
      "    \"I-Strength\": 17,\n",
      "    \"O\": 18\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file xlmr_ner.model/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing XLMRobertaForTokenClassification.\n",
      "\n",
      "All the weights of XLMRobertaForTokenClassification were initialized from the model checkpoint at xlmr_ner.model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForTokenClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"ner\", model=\"xlmr_ner.model\", tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "93bd6aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "tekstike = \" \".join([x[0] for x in output if len(x) > 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "31517cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array(['B-Drug', 'B-Reason', 'I-Reason'], dtype='<U8'), array([36,  7,  3]))\n"
     ]
    }
   ],
   "source": [
    "test_text = \" \".join([x[0] for x in test_total[1]])\n",
    "res = pipe(test_text)\n",
    "print(np.unique(np.array([x['entity'] for x in res]), return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a7bec989",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = \" \".join([x[0] for x in test_total[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1cde59dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pipe(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2aab4f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entity': 'B-ADE', 'score': 0.99907255, 'index': 1, 'word': '▁Tu', 'start': 0, 'end': 2}\n",
      "{'entity': 'B-ADE', 'score': 0.9994235, 'index': 2, 'word': 'imus', 'start': 2, 'end': 6}\n",
      "{'entity': '0', 'score': 0.9999534, 'index': 3, 'word': '▁es', 'start': 7, 'end': 9}\n",
      "{'entity': '0', 'score': 0.9999505, 'index': 4, 'word': 'ci', 'start': 9, 'end': 11}\n",
      "{'entity': '0', 'score': 0.9999536, 'index': 5, 'word': 'talo', 'start': 11, 'end': 15}\n",
      "{'entity': '0', 'score': 0.9999602, 'index': 6, 'word': 'pra', 'start': 15, 'end': 18}\n",
      "{'entity': '0', 'score': 0.9999417, 'index': 7, 'word': 'mist', 'start': 18, 'end': 22}\n",
      "{'entity': '0', 'score': 0.9999856, 'index': 8, 'word': '▁Pro', 'start': 23, 'end': 26}\n",
      "{'entity': '0', 'score': 0.99998724, 'index': 9, 'word': 'ovi', 'start': 26, 'end': 29}\n",
      "{'entity': '0', 'score': 0.9999887, 'index': 10, 'word': 'da', 'start': 29, 'end': 31}\n",
      "{'entity': '0', 'score': 0.999987, 'index': 11, 'word': '▁5', 'start': 32, 'end': 33}\n",
      "{'entity': '0', 'score': 0.999987, 'index': 12, 'word': 'mg', 'start': 33, 'end': 35}\n",
      "{'entity': '0', 'score': 0.9999722, 'index': 13, 'word': '▁es', 'start': 36, 'end': 38}\n",
      "{'entity': '0', 'score': 0.99997354, 'index': 14, 'word': 'cita', 'start': 38, 'end': 42}\n",
      "{'entity': '0', 'score': 0.9999745, 'index': 15, 'word': 'lop', 'start': 42, 'end': 45}\n",
      "{'entity': '0', 'score': 0.99996555, 'index': 16, 'word': 'rami', 'start': 45, 'end': 49}\n",
      "{'entity': '0', 'score': 0.99998796, 'index': 17, 'word': '▁pidevalt', 'start': 50, 'end': 58}\n",
      "{'entity': '0', 'score': 0.99997854, 'index': 18, 'word': '▁Ami', 'start': 59, 'end': 62}\n",
      "{'entity': '0', 'score': 0.9999814, 'index': 19, 'word': 'trip', 'start': 62, 'end': 66}\n",
      "{'entity': '0', 'score': 0.9999778, 'index': 20, 'word': 'ty', 'start': 66, 'end': 68}\n",
      "{'entity': '0', 'score': 0.9999796, 'index': 21, 'word': 'lin', 'start': 68, 'end': 71}\n",
      "{'entity': '0', 'score': 0.9999877, 'index': 22, 'word': '▁jätku', 'start': 72, 'end': 77}\n",
      "{'entity': '0', 'score': 0.9999896, 'index': 23, 'word': 'b', 'start': 77, 'end': 78}\n",
      "{'entity': '0', 'score': 0.99998796, 'index': 24, 'word': '▁sama', 'start': 79, 'end': 83}\n",
      "{'entity': '0', 'score': 0.9999883, 'index': 25, 'word': 'moodi', 'start': 83, 'end': 88}\n",
      "{'entity': '0', 'score': 0.9999888, 'index': 26, 'word': '▁P', 'start': 89, 'end': 90}\n",
      "{'entity': '0', 'score': 0.99998796, 'index': 27, 'word': 's', 'start': 90, 'end': 91}\n",
      "{'entity': '0', 'score': 0.9999887, 'index': 28, 'word': 'üh', 'start': 91, 'end': 93}\n",
      "{'entity': '0', 'score': 0.9999875, 'index': 29, 'word': 'ho', 'start': 93, 'end': 95}\n",
      "{'entity': '0', 'score': 0.99998736, 'index': 30, 'word': 'tera', 'start': 95, 'end': 99}\n",
      "{'entity': '0', 'score': 0.9999881, 'index': 31, 'word': 'a', 'start': 99, 'end': 100}\n",
      "{'entity': '0', 'score': 0.99998796, 'index': 32, 'word': 'pia', 'start': 100, 'end': 103}\n",
      "{'entity': '0', 'score': 0.99998164, 'index': 33, 'word': '▁Tu', 'start': 104, 'end': 106}\n",
      "{'entity': '0', 'score': 0.9999844, 'index': 34, 'word': 'imus', 'start': 106, 'end': 110}\n",
      "{'entity': 'B-Drug', 'score': 0.70192504, 'index': 35, 'word': '▁es', 'start': 111, 'end': 113}\n",
      "{'entity': 'B-Drug', 'score': 0.708226, 'index': 36, 'word': 'ci', 'start': 113, 'end': 115}\n",
      "{'entity': 'B-Drug', 'score': 0.70337576, 'index': 37, 'word': 'talo', 'start': 115, 'end': 119}\n",
      "{'entity': 'B-Drug', 'score': 0.7415874, 'index': 38, 'word': 'pra', 'start': 119, 'end': 122}\n",
      "{'entity': 'B-Drug', 'score': 0.71758306, 'index': 39, 'word': 'mist', 'start': 122, 'end': 126}\n",
      "{'entity': '0', 'score': 0.9999833, 'index': 40, 'word': '▁Pro', 'start': 127, 'end': 130}\n",
      "{'entity': '0', 'score': 0.99998605, 'index': 41, 'word': 'ovi', 'start': 130, 'end': 133}\n",
      "{'entity': '0', 'score': 0.9999871, 'index': 42, 'word': 'da', 'start': 133, 'end': 135}\n",
      "{'entity': '0', 'score': 0.99998367, 'index': 43, 'word': '▁5', 'start': 136, 'end': 137}\n",
      "{'entity': '0', 'score': 0.99998546, 'index': 44, 'word': 'mg', 'start': 137, 'end': 139}\n",
      "{'entity': '0', 'score': 0.7904959, 'index': 45, 'word': '▁es', 'start': 140, 'end': 142}\n",
      "{'entity': '0', 'score': 0.79297745, 'index': 46, 'word': 'cita', 'start': 142, 'end': 146}\n",
      "{'entity': '0', 'score': 0.804013, 'index': 47, 'word': 'lop', 'start': 146, 'end': 149}\n",
      "{'entity': '0', 'score': 0.788127, 'index': 48, 'word': 'rami', 'start': 149, 'end': 153}\n",
      "{'entity': '0', 'score': 0.9999851, 'index': 49, 'word': '▁pidevalt', 'start': 154, 'end': 162}\n",
      "{'entity': '0', 'score': 0.9985576, 'index': 50, 'word': '▁Ami', 'start': 163, 'end': 166}\n",
      "{'entity': '0', 'score': 0.9994035, 'index': 51, 'word': 'trip', 'start': 166, 'end': 170}\n",
      "{'entity': '0', 'score': 0.9994814, 'index': 52, 'word': 'ty', 'start': 170, 'end': 172}\n",
      "{'entity': '0', 'score': 0.9980083, 'index': 53, 'word': 'lin', 'start': 172, 'end': 175}\n",
      "{'entity': '0', 'score': 0.9999856, 'index': 54, 'word': '▁jätku', 'start': 176, 'end': 181}\n",
      "{'entity': '0', 'score': 0.9999869, 'index': 55, 'word': 'b', 'start': 181, 'end': 182}\n",
      "{'entity': '0', 'score': 0.9999864, 'index': 56, 'word': '▁sama', 'start': 183, 'end': 187}\n",
      "{'entity': '0', 'score': 0.9999857, 'index': 57, 'word': 'moodi', 'start': 187, 'end': 192}\n",
      "{'entity': '0', 'score': 0.9999846, 'index': 58, 'word': '▁P', 'start': 193, 'end': 194}\n",
      "{'entity': '0', 'score': 0.9999845, 'index': 59, 'word': 's', 'start': 194, 'end': 195}\n",
      "{'entity': '0', 'score': 0.99998486, 'index': 60, 'word': 'üh', 'start': 195, 'end': 197}\n",
      "{'entity': '0', 'score': 0.9999826, 'index': 61, 'word': 'ho', 'start': 197, 'end': 199}\n",
      "{'entity': '0', 'score': 0.9999809, 'index': 62, 'word': 'tera', 'start': 199, 'end': 203}\n",
      "{'entity': '0', 'score': 0.9999771, 'index': 63, 'word': 'a', 'start': 203, 'end': 204}\n",
      "{'entity': '0', 'score': 0.99993336, 'index': 64, 'word': 'pia', 'start': 204, 'end': 207}\n"
     ]
    }
   ],
   "source": [
    "# when they were 0\n",
    "for i in res:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3a757b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = \" \".join([x[0] for x in test_total[4]])\n",
    "res = pipe(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "14e3fe2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_spans_from_ner(ner_results: str):\n",
    "    total = len(ner_results)\n",
    "    counter = 0\n",
    "    results = []\n",
    "\n",
    "    while counter < total:\n",
    "        spanstart = ner_results[counter]['start']\n",
    "        spanend = ner_results[counter]['end']\n",
    "        word = ner_results[counter]['word']\n",
    "        entity = ner_results[counter]['entity']\n",
    "\n",
    "        nextword = word\n",
    "\n",
    "        for i in range(counter+1, total):\n",
    "            if ner_results[i]['word'].startswith('##'):\n",
    "                nextword += ner_results[i]['word']\n",
    "                spanend = ner_results[i]['end']\n",
    "                if ner_results[i]['entity'] != '0' and entity == '0':\n",
    "                    entity = ner_results[i]['entity']\n",
    "                counter += 1\n",
    "            else:\n",
    "                break\n",
    "        results.append({'word': nextword.replace(\"##\", \"\"), 'start': spanstart, 'end': spanend, 'entity': entity})\n",
    "        counter += 1\n",
    "\n",
    "    return results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
